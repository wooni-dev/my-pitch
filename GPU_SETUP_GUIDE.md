# GPU ì„¤ì • ê°€ì´ë“œ

Docker ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ NVIDIA GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì • ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ì‚¬ì „ ìš”êµ¬ì‚¬í•­](#ì‚¬ì „-ìš”êµ¬ì‚¬í•­)
- [NVIDIA Container Toolkit ì„¤ì¹˜](#nvidia-container-toolkit-ì„¤ì¹˜)
- [Docker Compose ì„¤ì •](#docker-compose-ì„¤ì •)
- [ì„¤ì¹˜ í™•ì¸](#ì„¤ì¹˜-í™•ì¸)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### 1. NVIDIA GPU ë“œë¼ì´ë²„ ì„¤ì¹˜ í™•ì¸

```bash
nvidia-smi
```

ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì´ ë‚˜ì™€ì•¼ í•©ë‹ˆë‹¤:

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.261.03             Driver Version: 535.261.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
...
```

- **Driver Version**: NVIDIA ë“œë¼ì´ë²„ ë²„ì „
- **CUDA Version**: ì§€ì›ë˜ëŠ” ìµœëŒ€ CUDA ë²„ì „ (ì´ ë²„ì „ ì´í•˜ì˜ CUDAë¥¼ ì‚¬ìš© ê°€ëŠ¥)

### 2. Docker ë° Docker Compose ì„¤ì¹˜ í™•ì¸

```bash
docker --version
docker compose version
```

Docker Compose v1.28.0 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤ (GPU ì„¤ì • ì§€ì›).

---

## NVIDIA Container Toolkit ì„¤ì¹˜

### 1. GPG í‚¤ ì¶”ê°€

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
    sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
```

### 2. ì €ì¥ì†Œ ì¶”ê°€ (Ubuntu/Debian)

```bash
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
```

### 4. Dockerì— GPU ëŸ°íƒ€ì„ ì„¤ì •

```bash
sudo nvidia-ctk runtime configure --runtime=docker
```

ì´ ëª…ë ¹ì€ `/etc/docker/daemon.json` íŒŒì¼ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ NVIDIA ëŸ°íƒ€ì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

### 5. Docker ì¬ì‹œì‘

```bash
sudo systemctl restart docker
```

---

## Docker Compose ì„¤ì •

### GPU ì„¤ì • ì¶”ê°€

`docker-compose.prod.yml` (ë˜ëŠ” `docker-compose.yml`)ì—ì„œ GPUë¥¼ ì‚¬ìš©í•  ì„œë¹„ìŠ¤ì— ë‹¤ìŒ ì„¤ì •ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

```yaml
services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile.prod
    container_name: my-pitch-api
    
    # ... ê¸°ì¡´ ì„¤ì • ...
    
    # GPU ì„¤ì •
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # ëª¨ë“  GPU ì‚¬ìš© (íŠ¹ì • ê°œìˆ˜ ì§€ì •: 1, 2 ë“±)
              capabilities: [gpu]
    
    # ... ê¸°íƒ€ ì„¤ì • ...
```

### GPU ê°œìˆ˜ ì œí•œ (ì„ íƒì‚¬í•­)

íŠ¹ì • ê°œìˆ˜ì˜ GPUë§Œ ì‚¬ìš©í•˜ë ¤ë©´:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1  # GPU 1ê°œë§Œ ì‚¬ìš©
          capabilities: [gpu]
```

### íŠ¹ì • GPU ì„ íƒ (ì„ íƒì‚¬í•­)

íŠ¹ì • GPU IDë¥¼ ì§€ì •í•˜ë ¤ë©´:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0']  # GPU 0ë²ˆë§Œ ì‚¬ìš©
          capabilities: [gpu]
```

---

## ì„¤ì¹˜ í™•ì¸

### 1. Docker GPU í…ŒìŠ¤íŠ¸

CUDA ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ì—¬ GPU ì¸ì‹ í™•ì¸:

```bash
# CUDA 12.2 í…ŒìŠ¤íŠ¸ (ë“œë¼ì´ë²„ê°€ ì§€ì›í•˜ëŠ” ë²„ì „ ì‚¬ìš©)
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

# ë˜ëŠ” CUDA 11.8 í…ŒìŠ¤íŠ¸ (PyTorch ë“± ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‚¬ìš©)
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

ì„±ê³µí•˜ë©´ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ `nvidia-smi` ì¶œë ¥ì´ í‘œì‹œë©ë‹ˆë‹¤.

### 2. í”„ë¡œë•ì…˜ í™˜ê²½ ì‹¤í–‰

```bash
docker compose -f docker-compose.prod.yml up -d --build
```

### 3. API ì»¨í…Œì´ë„ˆì—ì„œ GPU í™•ì¸

```bash
docker exec -it my-pitch-api nvidia-smi
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: `could not select device driver "nvidia" with capabilities: [[gpu]]`

**ì›ì¸**: NVIDIA Container Toolkitì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ Dockerê°€ ì¬ì‹œì‘ë˜ì§€ ì•ŠìŒ

**í•´ê²°ë°©ë²•**:
```bash
# Toolkit ì„¤ì¹˜ í™•ì¸
dpkg -l | grep nvidia-container-toolkit

# Docker ì¬ì‹œì‘
sudo systemctl restart docker

# í…ŒìŠ¤íŠ¸
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### ë¬¸ì œ 2: `nvidia-smi: command not found` (í˜¸ìŠ¤íŠ¸ì—ì„œ)

**ì›ì¸**: NVIDIA GPU ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ

**í•´ê²°ë°©ë²•**:
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y nvidia-driver-535
sudo reboot
```

### ë¬¸ì œ 3: Nginx ì„¤ì • ì˜¤ë¥˜ - `named location can be on the server level only`

**ì›ì¸**: Named location (`@...`)ì´ ì˜ëª»ëœ ì¤‘ì²© ë ˆë²¨ì— ì •ì˜ë¨

**í•´ê²°ë°©ë²•**: Named locationì„ `location` ë¸”ë¡ ì•ˆì´ ì•„ë‹Œ `server` ë¸”ë¡ ë ˆë²¨ì— ì •ì˜

**ì˜ëª»ëœ ì˜ˆ**:
```nginx
server {
    location / {
        error_page 413 @request_entity_too_large;
        
        location @request_entity_too_large {  # âŒ ì¤‘ì²©ëœ location ì•ˆì— ìˆìŒ
            return 413 '{"message": "..."}';
        }
    }
}
```

**ì˜¬ë°”ë¥¸ ì˜ˆ**:
```nginx
server {
    location / {
        error_page 413 @request_entity_too_large;
    }
    
    location @request_entity_too_large {  # âœ… server ë ˆë²¨ì— ì •ì˜
        return 413 '{"message": "..."}';
    }
}
```

### ë¬¸ì œ 4: Docker Compose GPU ì„¤ì •ì´ ì¸ì‹ë˜ì§€ ì•ŠìŒ

**ì›ì¸**: Docker Compose ë²„ì „ì´ ë„ˆë¬´ ë‚®ìŒ (v1.28.0 ë¯¸ë§Œ)

**í•´ê²°ë°©ë²•**:
```bash
# Docker Compose ë²„ì „ í™•ì¸
docker compose version

# í•„ìš”ì‹œ Docker Compose ì—…ë°ì´íŠ¸
sudo apt-get update
sudo apt-get install docker-compose-plugin
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### CUDA ë²„ì „ ì„ íƒ ê°€ì´ë“œ

| ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ | ê¶Œì¥ CUDA ë²„ì „ | Docker ì´ë¯¸ì§€ ì˜ˆì‹œ |
|---|---|---|
| PyTorch 2.x | 11.8 ë˜ëŠ” 12.1 | `pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime` |
| TensorFlow 2.12-2.13 | 11.8 | `tensorflow/tensorflow:2.13.0-gpu` |
| ìµœì‹  CUDA ê¸°ëŠ¥ | 12.2+ | `nvidia/cuda:12.2.0-cudnn8-runtime-ubuntu22.04` |

### Dockerfileì—ì„œ GPU ì§€ì› ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì‚¬ìš©

```dockerfile
# CUDA ëŸ°íƒ€ì„ ì´ë¯¸ì§€
FROM nvidia/cuda:12.2.0-cudnn8-runtime-ubuntu22.04

# ë˜ëŠ” PyTorch ê³µì‹ ì´ë¯¸ì§€
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë“±...
```

### ìœ ìš©í•œ ëª…ë ¹ì–´

```bash
# GPU ì‚¬ìš© ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
nvidia-smi

# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ GPU í™•ì¸
docker exec -it my-pitch-api nvidia-smi

# Docker GPU ëŸ°íƒ€ì„ ì„¤ì • í™•ì¸
cat /etc/docker/daemon.json
```

---

## ğŸ’¡ ì¶”ê°€ íŒ

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (PyTorch)

ì½”ë“œì—ì„œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì œí•œí•˜ë ¤ë©´:

```python
import torch

# GPU ë©”ëª¨ë¦¬ ì œí•œ (ì˜ˆ: ìµœëŒ€ 8GB)
torch.cuda.set_per_process_memory_fraction(0.5)  # 50% ì‚¬ìš©

# ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
# PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### í™˜ê²½ ë³€ìˆ˜ë¡œ GPU ì„ íƒ

```yaml
environment:
  - CUDA_VISIBLE_DEVICES=0  # GPU 0ë²ˆë§Œ ì‚¬ìš©
  # ë˜ëŠ”
  - CUDA_VISIBLE_DEVICES=0,1  # GPU 0ë²ˆ, 1ë²ˆ ì‚¬ìš©
```

---

## âœ… ì„¤ì • ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `nvidia-smi` ëª…ë ¹ ì •ìƒ ì‹¤í–‰
- [ ] NVIDIA Container Toolkit ì„¤ì¹˜ ì™„ë£Œ
- [ ] Docker GPU ëŸ°íƒ€ì„ ì„¤ì • ì™„ë£Œ
- [ ] Docker ì¬ì‹œì‘ ì™„ë£Œ
- [ ] `docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi` í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] `docker-compose.prod.yml`ì— GPU ì„¤ì • ì¶”ê°€
- [ ] í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ GPU ì¸ì‹ í™•ì¸

ëª¨ë“  í•­ëª©ì´ ì²´í¬ë˜ë©´ GPU ì„¤ì •ì´ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤! ğŸ‰

